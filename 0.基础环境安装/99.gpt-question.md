
==========
1.如果安装了多个wsl，这几个在同一时刻是都在运行着吗？
2.这样的系统和宿主机是NAT吗？
4.常用命令
5.win10安装多个版本命令
6.pycharm   ubuntu  wsl  
7.anaconda wsl
8.在wsl中安装的ubuntu系统，里面安装的软件能有可视化界面吗？

9.我在win10中使用wsl安装了ubuntu20.04, 那么使用pycharm开发python相关项目，pycharm是安装在wsl中还是安装在宿主机win10中？anaconda是安装在wsl中还是宿主机中？
10.算法模型是怎么在生产环境中落地的？直接静态的模型数据放上去，通过脚本调用吗？
11.wsl 安装k8s？]
12.一般深度学习时，使用wsl ubuntu调试代码，使用桥接还是其他网络模式更方便
13.anaconda安装好后，本身就自带很多python版本嘛？
14.wsl2系统设置cpu，内存等，多少比较合理不会卡 
15.pycharm使用wsl中conda,如何导包。
16.conda中如何下载某些包，如何设置源。
17.一般情况下，生产环境中算法模型是放在容器中，还是直接部署在宿主机？
18.conda可以用来管理什么，只能是python的相关包嘛？
19.win10我安装了pycharm，同时使用wsl2安装了ubuntu, ubuntu中安装了anaconda, 我现在想使用jupyter notebook, 怎么安装jupyter比较好，怎么调试方便？
20.conda install jupyter  这句命令有什么作用，安装的jupyter是独立运行在conda的某个环境中嘛？
19.pycharm哪些好用的算法工具插件
21.wsl2不太稳定，好像会经常挂掉
22.目前比较流行的微服务框架，或者单体也行，扩展性很好。
23.目标检测追踪是什么算法？摄像头跟踪，实时识别出人或物体？抛洒物
24.从高中开始，应该学哪些数学知识，从易到难，比如高等数学，线性代数，泛函、群论等，应该从哪些基础开始学，学习路线？
25.一般的海康摄像头本身内置有各种算法，实际上是不是就是摄像头硬件里面放了多个静态的模型文件？
26.深度神经网络可以进行特征学习和模式识别，能够从原始数据中自动提取特征，这句话中“自动提取特征”是什么意思？
27.深度神经网络可以自动提取特征，但是训练时的结果集还是得人为标注的吧？
28.图像分类中训练好的模型， 如果在实际使用场景中，图片的背景发生变化，例如原来背景是墙壁，现在变成森林， 对预测结果影响大吗？换句话说，模型是否只能应用在同一图片背景下？
29.深度学习，神经网络算法，监督学习，CNN,RNN、迁移学习，GAN，pytorch等机器学习分类概念，我很混乱，能给我列一下这些分类的种类和相互关系，例如哪个属于哪个的子类，包含哪些分类和算法？
30.wm也就是说，如果训练的数据集比较少，训练出来的可靠性就很低。比如，我想实现施工场地下摄像头图片实时识别有人吸烟、没穿反光背心，人员聚集，必须先拿到大量的数据集才行，而且不同的工地场景下，可能不适用
31.wm一般的海康摄像头本身内置有各种算法，实际上是不是就是摄像头硬件里面放了多个静态的模型文件？
32.识别打斗不是应该是连续的照片吗，又是利用了什么算法？是根据每一帧来判断？
33.人脸识别和猫狗识别，算法是否一样？
34.哪些有名的免费训练集网站，如kaggle等。
35.泛函的学习路线，属于什么学科？和微积分什么关系？
36.生产环境中，如果模型已经部署上去了，除非人为重新训练新模型，否则旧模型是否就一直保持不变了？是否有算法可以让模型自升级进化？
37.大语言模型的损失函数是什么，也是通过梯度下降求得模型参数的吗？Adam优化器比梯度下降好在哪里？
38.强化学习是什么，和神经网络有什么区别，两者什么关系？
39.目前流行的大语言模型，比如chatgpt，是什么原理？为什么这么像真人的回答了？内部结构就是模拟的神经元吗？然后损失函数，梯度下降不断优化出来的吗？
40.卷积的物理意义是什么，又是在大学数学中哪本书（哪个科目，如高等数学。。。。）里面的？以高中的数学水平，应该怎么入门？
41.可不可以这样理解，卷积核小矩阵，在滑动时，类似于人眼，慢慢扫过整个图片， 而最终结果就是把整张图片的内容记下来了，其实最终结果就是每次扫描的叠加。而扫描滑动的步长越短，说明记录得越详细？
42.卷积核只有3*3 或5*5， 对于一张照片来说，其实是很小很小的的几个像素，为什么就能作为特征呢？不同的照片是很可能包含相同的像素吧，几率是多少呢？
43.梯度下降求最小值时，如何避免掉入局部的最小值？
44.迁移学习，可以理解为，初始化矩阵时，使用预训练好的矩阵参数，然后，通过训练新的训练集，调整参数获取最小损失，从而更新模型参数， 也就是说新的模型文件把旧的模型文件替换了。
   那么，在线学习的流程是怎样的？在线学习的过程中，模型还有效吗，还是说训练期间功能失效，只有训练完，服务器内更新了模型后，才能对外开放功能呢，假如一边训练一边提供功能，不是会有性能消耗吗？是两个进程吗？
45.滤波器为什么是线性系统，什么是滤波器，原理是什么？
45.傅里叶变换频域时域动画。
46.什么叫线性时不变系统（LTI系统）？
47.卷积时，直观理解：将一个函数“翻转并平移”，然后通过求它们的积分加权组合。这相当于将两个信号逐点相乘并叠加，计算出“相似性”。为什么要翻转？卷积就像是用滤波器对输入信号“盖章”，产生正确的印记。
48.边缘检测：当卷积核设计为边缘检测核时，它会在图像中寻找明暗的变化，类似于我们眼睛察觉到物体的边缘。 这个卷积核是怎么设计的呢？为什么就能检测到明暗变化？Sobel核、Prewitt核、Laplacian核 
49."一张图像可能有许多条直线，卷积核可以识别其中的某条直线模式。这个直线可能在不同的图像中出现，因此无论它出现在图像的哪个位置，卷积核都可以识别出它。”， 但是输入矩阵不是已经把这条直线变成散乱的不连续的像素了嘛，怎么还可以识别出来？
50.cnn的卷积核和最终的权重矩阵是两个东西吧？
51.通滤波器实现原理，为什么可以把高频信号过滤掉
52.为什么从时域变换到频域，就可以去除不需要的频率成分？
53.信号处理中的卷积可以用于模糊处理，低高通滤波器。和cnn中是两种概念。
54.“当我们说“翻转”，意味着我们把滤波器的时域反向看，这样它能够与输入信号进行对齐。翻转可以理解为从 右到左 逐步“滑动”滤波器信号，检查输入信号每一个小区间的重叠。” 为什么是从右到左，而不是从左到右？
55.内积与卷积的区别
56.矩阵有哪些常用操作，能否每种操作给个具体的例子，同时描述一下每种操作的物理意义，并且描述一下实际使用场景
57.矩阵乘法、卷积运算、矩阵变换的区别。
58.”卷积神经网络的核心思想是 特征学习，即通过反向传播（梯度下降）不断调整卷积核的权重，使其能够从输入数据中学习到有用的特征（如边缘、纹理、形状等）。
   网络的训练过程使得卷积核的权重逐渐调整，以便提取出图像中最具区分性的特征。“  卷积核不是固定的吗？怎么也会调整？
59.springboot调用python算法，生产环境中工程上一般是怎么做的？
60.数字信号和模拟信号各自的优势
61.数字和模拟电路可以相互转化吗，涉及到哪些数学知识，转化之后有哪些优势和缺陷？Analog Digital convert (ADC)
62.边缘检测中，卷积核是不是就是带通滤波器？
63.傅里叶相乘e-j2pift，怎么得出来的？
64.边缘检测中，卷积核可不可以理解成是带通滤波器，被处理的图像是波，这个波可以被傅里叶分解，而卷积核可以把特定的波形过滤出来？
65.矩阵可以表示波函数吗，可以进行傅里叶变换吗？
66.在频域中，通过乘法操作，去除不需要的频率成分，这里的乘法操作指的是什么？具体是怎么实现的？从原理和硬件实现上又是怎么做的？
67.频域中进行滤波操作，就是通过对每个频率分量进行乘法操作。那么矩阵或者函数乘以什么会等于0？
68.所以电路控制，控制的就是输出波形，也就是信号。而每种信号都可以傅里叶，可以过滤不想要的，而过滤可以通过硬件电容电感等来实现。转换到数学上，就是对每个频率分量的乘法操作。也就是分析电路？？电路的硬件都可以抽象为对频率的操作。
69.卷积是不是本质上就是过滤
70.电流或波，用矩阵表示
71.矩阵求逆公式，请用多个例子表示，如2*2，3*3,2*3矩阵的逆。
72.单位矩阵的物理意义。
73.傅里叶变换（连续信号），离散傅里叶变换 DFT（离散信号） 和快速傅里叶变换 FFT（离散信号的高效实现）
74.python类似matlab的包，包括画图模拟等。

===============
卷积核就是一个过滤器
1.摄像头加速和优化：专用的AL加速芯片，如ASIC(专用集成电路)或者GPU。不同于PC，可能会用上TensorRT(NVIDIA的高效推理引擎)，OpenVINO(Intel的加速推理工具)。
2.场景多样性的训练集，可以采取1.**数据增强**：模拟不同光照不同角度，不同背景。2.**迁移学习**，利用其他相似条件下的预训练模型，进行微调，适应目标场景。
3.实时性问题：选用轻量级网络架构：YOLO,MobileNet,EfficientNet等，或者通过模型压缩，量化，提升推理速度。
4.小样本问题：考虑数据合成，或生成对抗网络GAN，通过虚拟数据来补充训练集。
5.如果背景比较单一：迁移学习，先在大规模通用数据集（如ImageNet）上训练一个预训练模型，然后在具体的场景上微调。
6.数据漂移：在线学习，迁移学习，自适应模型，强化学习，
7.避免梯度下降到局部最小值：随机初始化不同路径，使用动量，自适应学习率（Adagrad,RMSprop,Adam），随机梯度下降（每次更新只用部分数据，因为使用全量数据会朝相同的方向下降），
                       L-BFGS等基于牛顿法使用二阶信息，尽量使目标函数变成凸函数，正则化L1/L2正则化控制模型复杂度。
8.矩阵乘法、卷积运算、矩阵变换的区别。
9.卷积核，一般会有多组，其实就是卷积层的最终矩阵权重。当然，不包括后面的全连接层的权重
10.
=========

在线学习：SGD随机梯度下降，可以对参数进行在线更新。

===================================================LLM相关==============================================
1.ubuntu上vllm部署的流程，以及多台机子部署流程
2.langchain---langchat
3.hugginface transformers库怎么使用
4.python 使用vllm
5.vllm对cuda的版本是否得是cuda12.1以上
6.https://www.bilibili.com/video/BV1FgPNejEMM?spm_id_from=333.788.videopod.episodes&vd_source=8a92490bd4ee9fd50a3d74d697fd7f3a&p=2 langchain视频
7.langchain文档
8.langchain和pytorch有什么关系吗，定位一样吗？
9.pytorch 微调模型deepseek,python代码。训练数据去哪里拿？